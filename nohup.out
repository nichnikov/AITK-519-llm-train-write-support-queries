
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/AITK-519-llm-train-write-support-queries/llm_train.py", line 9, in <module>
    from transformers import Trainer, TrainingArguments
  File "/home/AITK-519-llm-train-write-support-queries/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1372, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/AITK-519-llm-train-write-support-queries/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1382, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/AITK-519-llm-train-write-support-queries/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 78, in <module>
    from .trainer_pt_utils import (
  File "/home/AITK-519-llm-train-write-support-queries/.venv/lib/python3.10/site-packages/transformers/trainer_pt_utils.py", line 218, in <module>
    device: Optional[torch.device] = torch.device("cuda"),
/home/AITK-519-llm-train-write-support-queries/.venv/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:218: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: Optional[torch.device] = torch.device("cuda"),
/home/AITK-519-llm-train-write-support-queries/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/AITK-519-llm-train-write-support-queries/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/AITK-519-llm-train-write-support-queries/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
  0%|          | 0/1326 [00:00<?, ?it/s]  0%|          | 1/1326 [00:08<3:15:46,  8.86s/it]  0%|          | 2/1326 [00:17<3:06:38,  8.46s/it]  0%|          | 3/1326 [00:25<3:03:50,  8.34s/it]  0%|          | 4/1326 [00:33<3:02:42,  8.29s/it]  0%|          | 5/1326 [00:41<3:02:04,  8.27s/it]  0%|          | 6/1326 [00:49<3:01:48,  8.26s/it]  1%|          | 7/1326 [00:58<3:01:38,  8.26s/it]  1%|          | 8/1326 [01:06<3:01:31,  8.26s/it]  1%|          | 9/1326 [01:14<3:01:24,  8.26s/it]  1%|          | 10/1326 [01:23<3:01:19,  8.27s/it]  1%|          | 11/1326 [01:31<3:01:15,  8.27s/it]  1%|          | 12/1326 [01:39<3:01:17,  8.28s/it]  1%|          | 13/1326 [01:47<3:01:17,  8.28s/it]  1%|          | 14/1326 [01:56<3:01:14,  8.29s/it]  1%|          | 15/1326 [02:04<3:01:10,  8.29s/it]  1%|          | 16/1326 [02:12<3:01:09,  8.30s/it]  1%|▏         | 17/1326 [02:21<3:01:01,  8.30s/it]  1%|▏         | 18/1326 [02:29<3:00:58,  8.30s/it]  1%|▏         | 19/1326 [02:37<3:00:54,  8.30s/it]  2%|▏         | 20/1326 [02:46<3:00:43,  8.30s/it]  2%|▏         | 21/1326 [02:54<3:00:34,  8.30s/it]  2%|▏         | 22/1326 [03:02<3:00:25,  8.30s/it]  2%|▏         | 23/1326 [03:10<3:00:17,  8.30s/it]  2%|▏         | 24/1326 [03:19<3:00:12,  8.30s/it]  2%|▏         | 25/1326 [03:27<3:00:01,  8.30s/it]  2%|▏         | 26/1326 [03:35<2:59:51,  8.30s/it]  2%|▏         | 27/1326 [03:44<2:59:43,  8.30s/it]  2%|▏         | 28/1326 [03:52<2:59:40,  8.31s/it]  2%|▏         | 29/1326 [04:00<2:59:35,  8.31s/it]  2%|▏         | 30/1326 [04:09<2:59:28,  8.31s/it]  2%|▏         | 31/1326 [04:17<2:59:20,  8.31s/it]  2%|▏         | 32/1326 [04:25<2:59:17,  8.31s/it]  2%|▏         | 33/1326 [04:34<2:59:11,  8.31s/it]  3%|▎         | 34/1326 [04:42<2:59:00,  8.31s/it]  3%|▎         | 35/1326 [04:50<2:58:53,  8.31s/it]  3%|▎         | 36/1326 [04:58<2:58:45,  8.31s/it]  3%|▎         | 37/1326 [05:07<2:58:36,  8.31s/it]  3%|▎         | 38/1326 [05:15<2:58:23,  8.31s/it]  3%|▎         | 39/1326 [05:23<2:58:15,  8.31s/it]  3%|▎         | 40/1326 [05:32<2:58:07,  8.31s/it]  3%|▎         | 41/1326 [05:40<2:57:59,  8.31s/it]  3%|▎         | 42/1326 [05:48<2:57:55,  8.31s/it]  3%|▎         | 43/1326 [05:57<2:57:46,  8.31s/it]  3%|▎         | 44/1326 [06:05<2:57:39,  8.31s/it]  3%|▎         | 45/1326 [06:13<2:57:34,  8.32s/it]  3%|▎         | 46/1326 [06:22<2:57:35,  8.32s/it]  4%|▎         | 47/1326 [06:30<2:57:30,  8.33s/it]  4%|▎         | 48/1326 [06:38<2:57:18,  8.32s/it]